//go:build manual_api_test
// +build manual_api_test

// Package e2e contains end-to-end tests for the thinktank CLI
// These tests require a valid API key to run properly and are skipped by default
// To run these tests: go test -tags=manual_api_test ./internal/e2e/...
package e2e

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"net/http/httptest"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"strings"
	"testing"

	"github.com/phrazzld/thinktank/internal/logutil"
)

// thinktankBinaryPath stores the path to the compiled binary, set once in TestMain
var thinktankBinaryPath string

// logger provides structured logging for E2E tests
var logger logutil.LoggerInterface

const (
	// Mock API responses
	mockTokenCount          = 1000
	mockGeneratedContent    = "# Test Generated Plan\n\nThis is a test plan generated by the mock API."
	mockGenerationTokenUsed = 500
	mockFinishReason        = "STOP"
)

// mockGeminiError represents the structure of a Gemini API error response
type mockGeminiError struct {
	Error struct {
		Message    string `json:"message"`
		StatusCode int    `json:"statusCode"`
		Suggestion string `json:"suggestion"`
	} `json:"error"`
}

// testFlags holds predefined flags for convenient reuse in tests
type testFlags struct {
	Instructions   string
	OutputDir      string
	ModelNames     []string
	Model          []string
	APIKey         string
	Include        string
	Exclude        string
	ExcludeNames   string
	DryRun         bool
	ConfirmTokens  int
	Verbose        bool
	LogLevel       string
	AuditLogFile   string
	ForceOverwrite bool
}

// TestEnv represents the test environment for e2e tests
type TestEnv struct {
	// Testing context
	t *testing.T

	// Temporary directory for the test
	TempDir string

	// Compiled binary path
	BinaryPath string

	// Mock HTTP server for the Gemini API
	MockServer *httptest.Server

	// Configuration for the mock server
	MockConfig struct {
		// Function to handle token counting requests
		HandleTokenCount func(req *http.Request) (int, error)

		// Function to handle generation requests
		HandleGeneration func(req *http.Request) (string, int, string, error)

		// Function to handle model info requests
		HandleModelInfo func(req *http.Request) (string, int, int, error)
	}

	// Default test flags
	DefaultFlags testFlags
}

// NewTestEnv creates a new test environment
func NewTestEnv(t *testing.T) *TestEnv {
	t.Helper()

	// Create a temporary directory for the test
	tempDir := t.TempDir()

	// Ensure the binary path was set by TestMain
	if thinktankBinaryPath == "" {
		// This should not happen if TestMain ran correctly
		t.Fatal("FATAL: thinktankBinaryPath not set by TestMain. E2E test setup failed.")
	}

	// Initialize the test environment using the binary path set by TestMain
	env := &TestEnv{
		t:          t,
		TempDir:    tempDir,
		BinaryPath: thinktankBinaryPath,
	}

	// Set up default mock handlers
	env.MockConfig.HandleTokenCount = func(req *http.Request) (int, error) {
		return mockTokenCount, nil
	}

	env.MockConfig.HandleGeneration = func(req *http.Request) (string, int, string, error) {
		return mockGeneratedContent, mockGenerationTokenUsed, mockFinishReason, nil
	}

	env.MockConfig.HandleModelInfo = func(req *http.Request) (string, int, int, error) {
		return "gemini-model", 100000, 8192, nil
	}

	// Start the mock server
	env.startMockServer()

	// Set up default test flags
	env.DefaultFlags = testFlags{
		Instructions:  "", // Will be set by specific tests
		OutputDir:     filepath.Join(tempDir, "output"),
		ModelNames:    []string{"gemini-2.5-pro-preview-03-25"}, // Use model from registry
		Model:         []string{"gemini-2.5-pro-preview-03-25"}, // Use model from registry
		APIKey:        "test-api-key",
		Include:       "",
		Exclude:       ".git,node_modules",
		ExcludeNames:  ".git,node_modules",
		DryRun:        false,
		ConfirmTokens: 0,
		Verbose:       false,
		LogLevel:      "info",
		AuditLogFile:  "",
	}

	return env
}

// Cleanup performs cleanup operations after the test
func (e *TestEnv) Cleanup() {
	// Shutdown the mock server if it exists
	if e.MockServer != nil {
		e.MockServer.Close()
	}
}

// startMockServer starts a mock HTTP server for all supported API providers (Gemini, OpenAI, OpenRouter)
func (e *TestEnv) startMockServer() {
	handler := http.NewServeMux()

	// ---- GEMINI API MOCKS ----

	// Handler for Gemini model info
	handler.HandleFunc("/v1/models/", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Return a valid model info response
		if err := json.NewEncoder(w).Encode(map[string]interface{}{
			"name":                       "gemini-test-model",
			"version":                    "001",
			"displayName":                "Test Model",
			"description":                "Test model for E2E tests",
			"inputTokenLimit":            100000,
			"outputTokenLimit":           8192,
			"supportedGenerationMethods": []string{"generateContent", "countTokens"},
		}); err != nil {
			e.t.Logf("Failed to encode model info response: %v", err)
		}
	})

	// Handle Gemini content generation requests - respond to all model variants
	modelPaths := []string{
		"/v1/models/gemini-2.5-pro-preview-03-25:generateContent",
		"/v1/models/gemini-2.5-flash-preview-04-17:generateContent",
		"/v1/models/gemini-pro:generateContent",
		"/v1/models/gemini-1.5-pro:generateContent",
	}

	for _, path := range modelPaths {
		localPath := path // Create local variable to avoid closure issues
		handler.HandleFunc(localPath, func(w http.ResponseWriter, r *http.Request) {
			content, tokens, finishReason, err := e.MockConfig.HandleGeneration(r)
			if err != nil {
				w.WriteHeader(http.StatusBadRequest)
				json.NewEncoder(w).Encode(map[string]interface{}{
					"error": map[string]interface{}{
						"code":    400,
						"message": err.Error(),
						"status":  "INVALID_ARGUMENT",
					},
				})
				return
			}

			w.Header().Set("Content-Type", "application/json")
			if err := json.NewEncoder(w).Encode(map[string]interface{}{
				"candidates": []map[string]interface{}{
					{
						"content": map[string]interface{}{
							"parts": []map[string]interface{}{
								{
									"text": content,
								},
							},
							"role": "model",
						},
						"finishReason": finishReason,
						"safetyRatings": []map[string]interface{}{
							{
								"category":    "HARM_CATEGORY_DANGEROUS_CONTENT",
								"probability": "NEGLIGIBLE",
							},
						},
					},
				},
				"promptFeedback": map[string]interface{}{
					"safetyRatings": []map[string]interface{}{
						{
							"category":    "HARM_CATEGORY_DANGEROUS_CONTENT",
							"probability": "NEGLIGIBLE",
						},
					},
				},
				"usageMetadata": map[string]interface{}{
					"promptTokenCount":     tokens / 2,
					"candidatesTokenCount": tokens / 2,
					"totalTokenCount":      tokens,
				},
			}); err != nil {
				e.t.Logf("Failed to encode generate content response: %v", err)
			}
		})
	}

	// Handle Gemini token counting requests - respond to all model variants
	tokenCountPaths := []string{
		"/v1/models/gemini-2.5-pro-preview-03-25:countTokens",
		"/v1/models/gemini-2.5-flash-preview-04-17:countTokens",
		"/v1/models/gemini-pro:countTokens",
		"/v1/models/gemini-1.5-pro:countTokens",
	}

	for _, path := range tokenCountPaths {
		localPath := path // Create local variable to avoid closure issues
		handler.HandleFunc(localPath, func(w http.ResponseWriter, r *http.Request) {
			count, err := e.MockConfig.HandleTokenCount(r)
			if err != nil {
				w.WriteHeader(http.StatusBadRequest)
				json.NewEncoder(w).Encode(map[string]interface{}{
					"error": map[string]interface{}{
						"code":    400,
						"message": err.Error(),
						"status":  "INVALID_ARGUMENT",
					},
				})
				return
			}

			w.Header().Set("Content-Type", "application/json")
			if err := json.NewEncoder(w).Encode(map[string]interface{}{
				"totalTokens": count,
			}); err != nil {
				e.t.Logf("Failed to encode count tokens response: %v", err)
			}
		})
	}

	// ---- OPENAI API MOCKS ----

	// Handler for OpenAI completions
	handler.HandleFunc("/v1/chat/completions", func(w http.ResponseWriter, r *http.Request) {
		content, tokens, finishReason, err := e.MockConfig.HandleGeneration(r)
		if err != nil {
			w.WriteHeader(http.StatusBadRequest)
			json.NewEncoder(w).Encode(map[string]interface{}{
				"error": map[string]interface{}{
					"message": err.Error(),
					"type":    "invalid_request_error",
					"code":    "invalid_argument",
				},
			})
			return
		}

		w.Header().Set("Content-Type", "application/json")
		if err := json.NewEncoder(w).Encode(map[string]interface{}{
			"id":      "chatcmpl-mock-id",
			"object":  "chat.completion",
			"created": 1679410928,
			"model":   "gpt-4",
			"choices": []map[string]interface{}{
				{
					"index": 0,
					"message": map[string]interface{}{
						"role":    "assistant",
						"content": content,
					},
					"finish_reason": strings.ToLower(finishReason),
				},
			},
			"usage": map[string]interface{}{
				"prompt_tokens":     tokens / 2,
				"completion_tokens": tokens / 2,
				"total_tokens":      tokens,
			},
		}); err != nil {
			e.t.Logf("Failed to encode OpenAI response: %v", err)
		}
	})

	// OpenAI models list endpoint
	handler.HandleFunc("/v1/models", func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		if err := json.NewEncoder(w).Encode(map[string]interface{}{
			"data": []map[string]interface{}{
				{
					"id":       "gpt-4.1",
					"object":   "model",
					"created":  1677610602,
					"owned_by": "openai",
				},
				{
					"id":       "o4-mini",
					"object":   "model",
					"created":  1677610602,
					"owned_by": "openai",
				},
			},
		}); err != nil {
			e.t.Logf("Failed to encode OpenAI models response: %v", err)
		}
	})

	// ---- OPENROUTER API MOCKS ----

	// Simplified OpenRouter completions endpoint
	handler.HandleFunc("/api/v1/chat/completions", func(w http.ResponseWriter, r *http.Request) {
		content, tokens, finishReason, err := e.MockConfig.HandleGeneration(r)
		if err != nil {
			w.WriteHeader(http.StatusBadRequest)
			json.NewEncoder(w).Encode(map[string]interface{}{
				"error": map[string]interface{}{
					"message": err.Error(),
					"type":    "invalid_request_error",
				},
			})
			return
		}

		w.Header().Set("Content-Type", "application/json")
		if err := json.NewEncoder(w).Encode(map[string]interface{}{
			"id":      "chatcmpl-openrouter-mock-id",
			"object":  "chat.completion",
			"created": 1679410928,
			"model":   "openrouter/meta-llama/llama-4-scout",
			"choices": []map[string]interface{}{
				{
					"index": 0,
					"message": map[string]interface{}{
						"role":    "assistant",
						"content": content,
					},
					"finish_reason": strings.ToLower(finishReason),
				},
			},
			"usage": map[string]interface{}{
				"prompt_tokens":     tokens / 2,
				"completion_tokens": tokens / 2,
				"total_tokens":      tokens,
			},
		}); err != nil {
			e.t.Logf("Failed to encode OpenRouter response: %v", err)
		}
	})

	// Create and start the mock server
	e.MockServer = httptest.NewServer(handler)
}

// CreateTestFile creates a file in the test environment with the given content
func (e *TestEnv) CreateTestFile(relativePath, content string) string {
	e.t.Helper()

	// Create the full path
	fullPath := filepath.Join(e.TempDir, relativePath)

	// Ensure parent directories exist
	dir := filepath.Dir(fullPath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		e.t.Fatalf("Failed to create directory %s: %v", dir, err)
	}

	// Write the file
	if err := os.WriteFile(fullPath, []byte(content), 0640); err != nil {
		e.t.Fatalf("Failed to write file %s: %v", fullPath, err)
	}

	return fullPath
}

// CreateTestDirectory creates a directory in the test environment
func (e *TestEnv) CreateTestDirectory(relativePath string) string {
	e.t.Helper()

	// Create the full path
	fullPath := filepath.Join(e.TempDir, relativePath)

	// Create the directory
	if err := os.MkdirAll(fullPath, 0755); err != nil {
		e.t.Fatalf("Failed to create directory %s: %v", fullPath, err)
	}

	return fullPath
}

// shouldSkipBinaryExecution returns true if we should skip actual binary execution
// This is useful in CI environments where binary execution may fail due to format issues
func shouldSkipBinaryExecution() bool {
	return os.Getenv("SKIP_BINARY_EXECUTION") == "true"
}

// RunThinktank runs the thinktank binary with the provided arguments
func (e *TestEnv) RunThinktank(args []string, stdin io.Reader) (stdout, stderr string, exitCode int, err error) {
	e.t.Helper()

	// Check if we should skip binary execution (for CI environments)
	if shouldSkipBinaryExecution() {
		e.t.Skip("Skipping binary execution test due to SKIP_BINARY_EXECUTION=true")
		return "", "", 0, nil
	}

	// Create buffers for stdout and stderr
	stdoutBuf := &bytes.Buffer{}
	stderrBuf := &bytes.Buffer{}

	// Create the command
	cmd := exec.Command(e.BinaryPath, args...)

	// Set up stdin, stdout, and stderr
	cmd.Stdin = stdin
	cmd.Stdout = stdoutBuf
	cmd.Stderr = stderrBuf

	// Set the working directory
	cmd.Dir = e.TempDir

	// Set up environment variables with mock server URL and API key
	cmd.Env = append(os.Environ(),
		// Always set a valid API key value for all providers
		"GEMINI_API_KEY=test-api-key",
		"OPENAI_API_KEY=test-api-key",
		"OPENROUTER_API_KEY=test-api-key",

		// Set the mock server URL as the API endpoint for all providers
		fmt.Sprintf("GEMINI_API_URL=%s", e.MockServer.URL),
		fmt.Sprintf("OPENAI_API_URL=%s", e.MockServer.URL),
		fmt.Sprintf("OPENROUTER_API_URL=%s", e.MockServer.URL),

		// Hard-code the model name to match mock server path
		"GEMINI_MODEL_NAME=gemini-2.5-pro-preview-03-25",
		"OPENAI_MODEL_NAME=o4-mini",
		"OPENROUTER_MODEL_NAME=openrouter/meta-llama/llama-4-scout",

		// Force env var source for all providers
		"GEMINI_API_KEY_SOURCE=env",
		"OPENAI_API_KEY_SOURCE=env",
		"OPENROUTER_API_KEY_SOURCE=env",

		// Debug mode for detailed logging
		"THINKTANK_DEBUG=true",

		// Explicitly set PATH to ensure binary can find dependencies
		fmt.Sprintf("PATH=%s", os.Getenv("PATH")),
	)

	// Print debug info
	e.t.Logf("Running %s with environment: GEMINI_API_URL=%s", e.BinaryPath, e.MockServer.URL)

	// Run the command
	runErr := cmd.Run()

	// Extract the exit code
	exitCode = 0
	if runErr != nil {
		if exitErr, ok := runErr.(*exec.ExitError); ok {
			exitCode = exitErr.ExitCode()
		} else {
			return stdoutBuf.String(), stderrBuf.String(), exitCode, fmt.Errorf("failed to run command: %v", runErr)
		}
	}

	// For debugging: log output on non-zero exit codes
	if exitCode != 0 {
		e.t.Logf("Command exited with code %d\nStdout: %s\nStderr: %s",
			exitCode, stdoutBuf.String(), stderrBuf.String())
	}

	return stdoutBuf.String(), stderrBuf.String(), exitCode, nil
}

// RunWithFlags runs the thinktank binary with the provided flags and additional arguments
func (e *TestEnv) RunWithFlags(flags testFlags, additionalArgs []string) (stdout, stderr string, exitCode int, err error) {
	e.t.Helper()

	// Construct arguments
	args := []string{}

	// Add flags
	if flags.Instructions != "" {
		instructionsFile := e.CreateTestFile("instructions.md", "Test instructions")
		args = append(args, "--instructions", instructionsFile)
	}

	if flags.OutputDir != "" {
		args = append(args, "--output-dir", flags.OutputDir)
	}

	// Use the new Model field if it's set, otherwise fall back to ModelNames
	if len(flags.Model) > 0 {
		for _, model := range flags.Model {
			args = append(args, "--model", model)
		}
	} else {
		for _, model := range flags.ModelNames {
			args = append(args, "--model", model)
		}
	}

	if flags.Include != "" {
		args = append(args, "--include", flags.Include)
	}

	if flags.Exclude != "" {
		args = append(args, "--exclude", flags.Exclude)
	}

	if flags.ExcludeNames != "" {
		args = append(args, "--exclude-names", flags.ExcludeNames)
	}

	if flags.DryRun {
		args = append(args, "--dry-run")
	}

	if flags.ConfirmTokens > 0 {
		args = append(args, "--confirm-tokens", fmt.Sprintf("%d", flags.ConfirmTokens))
	}

	if flags.Verbose {
		args = append(args, "--verbose")
	}

	if flags.LogLevel != "" {
		args = append(args, "--log-level", flags.LogLevel)
	}

	if flags.AuditLogFile != "" {
		args = append(args, "--audit-log-file", flags.AuditLogFile)
	}

	if flags.ForceOverwrite {
		args = append(args, "--force-overwrite")
	}

	// Add additional arguments
	args = append(args, additionalArgs...)

	// Run the command
	return e.RunThinktank(args, nil)
}

// findOrBuildBinary locates or builds the thinktank binary
func findOrBuildBinary() (string, error) {
	// Determine the project root directory relative to this test file
	projectRoot, err := os.Getwd() // Start from current dir
	if err != nil {
		return "", fmt.Errorf("failed to get current working directory: %v", err)
	}

	// If tests are run from within internal/e2e, go up two levels
	if strings.HasSuffix(projectRoot, "internal/e2e") {
		projectRoot = filepath.Dir(filepath.Dir(projectRoot))
	} else if strings.HasSuffix(projectRoot, "e2e") { // If run from e2e directly
		projectRoot = filepath.Dir(projectRoot)
	}
	// Additional logic could be added here to detect the project root more robustly
	// e.g., searching upward for go.mod file

	// Define potential binary locations relative to the project root
	binaryName := "thinktank"
	if os.PathSeparator == '\\' { // Handle Windows executable extension
		binaryName += ".exe"
	}

	candidates := []string{
		filepath.Join(projectRoot, binaryName),
		filepath.Join(projectRoot, "bin", binaryName),
		// Add additional common locations if needed
	}

	// Check for existing binary
	for _, candidate := range candidates {
		if _, err := os.Stat(candidate); err == nil {
			absPath, err := filepath.Abs(candidate)
			if err != nil {
				return "", fmt.Errorf("failed to get absolute path for %s: %v", candidate, err)
			}
			fmt.Printf("Found thinktank binary at: %s\n", absPath)
			return absPath, nil
		}
	}

	// If not found, build it
	fmt.Println("Thinktank binary not found, building from source...")
	buildOutput := filepath.Join(projectRoot, binaryName)

	// Build command targeting the main package - explicitly set to the host OS/arch
	fmt.Printf("Building binary for %s/%s\n", runtime.GOOS, runtime.GOARCH)

	// Build with explicit source files to avoid package conflicts with test files
	sourceFiles := []string{
		"cmd/thinktank/main.go",
		"cmd/thinktank/cli.go",
		"cmd/thinktank/api.go",
		"cmd/thinktank/output.go",
	}
	cmdArgs := append([]string{"build", "-o", buildOutput}, sourceFiles...)
	cmd := exec.Command("go", cmdArgs...)
	cmd.Dir = projectRoot // Ensure the build runs from the project root

	// Set appropriate environment variables to ensure binary is built for the current OS
	// This is critical for CI environments where the test might attempt to run with an incompatible binary
	cmd.Env = append(os.Environ(),
		"GOOS="+runtime.GOOS,
		"GOARCH="+runtime.GOARCH,
		"CGO_ENABLED=0", // Disable CGO for better cross-platform compatibility
	)
	stdout := &bytes.Buffer{}
	stderr := &bytes.Buffer{}
	cmd.Stdout = stdout
	cmd.Stderr = stderr

	if err := cmd.Run(); err != nil {
		return "", fmt.Errorf("failed to build thinktank binary: %v\nBuild Directory: %s\nStdout: %s\nStderr: %s",
			err, projectRoot, stdout.String(), stderr.String())
	}

	// Ensure the binary has executable permissions
	if err := os.Chmod(buildOutput, 0755); err != nil {
		return "", fmt.Errorf("failed to set executable permissions on binary: %v", err)
	}

	// Verify the binary was built
	if _, err := os.Stat(buildOutput); err != nil {
		return "", fmt.Errorf("failed to find built binary at %s after build attempt: %v", buildOutput, err)
	}

	absPath, err := filepath.Abs(buildOutput)
	if err != nil {
		return "", fmt.Errorf("failed to get absolute path for built binary: %v", err)
	}

	fmt.Printf("Successfully built thinktank binary at: %s\n", absPath)
	return absPath, nil
}

// FileExists checks if a file exists in the test environment
func (e *TestEnv) FileExists(relativePath string) bool {
	fullPath := filepath.Join(e.TempDir, relativePath)
	_, err := os.Stat(fullPath)
	return err == nil
}

// ReadFile reads a file from the test environment
func (e *TestEnv) ReadFile(relativePath string) (string, error) {
	fullPath := filepath.Join(e.TempDir, relativePath)
	content, err := os.ReadFile(fullPath)
	if err != nil {
		return "", err
	}
	return string(content), nil
}

// SimulateUserInput returns a reader that simulates user input
func SimulateUserInput(input string) io.Reader {
	return strings.NewReader(input)
}

// isRunningInGitHubActions returns true if running in GitHub Actions
func isRunningInGitHubActions() bool {
	return os.Getenv("GITHUB_ACTIONS") == "true"
}

// TestMain runs once before all tests in the package.
// It finds or builds the thinktank binary needed for the tests.
func TestMain(m *testing.M) {
	// Initialize structured JSON logger for E2E tests with correlation ID support
	ctx := logutil.WithCorrelationID(context.Background())
	logger = logutil.NewSlogLoggerFromLogLevel(os.Stderr, logutil.InfoLevel).WithContext(ctx)

	// Check if running in GitHub Actions
	if isRunningInGitHubActions() {
		logger.InfoContext(ctx, "Running in GitHub Actions environment")

		// In GitHub Actions, we expect the binary to be pre-built and symlinked by the workflow
		// The binary should be in the project root, not in the current directory
		projectRoot, err := filepath.Abs("../../") // Go up from internal/e2e to the project root
		if err != nil {
			logger.FatalContext(ctx, "Failed to get project root path", "error", err)
		}

		thinktankBinary := filepath.Join(projectRoot, "thinktank")
		if runtime.GOOS == "windows" {
			thinktankBinary += ".exe"
		}

		thinktankBinaryPath = thinktankBinary
		logger.InfoContext(ctx, "Using pre-built binary for E2E tests", "binary_path", thinktankBinaryPath)

		// Verify the binary exists and is executable
		if _, err := os.Stat(thinktankBinaryPath); err != nil {
			logger.FatalContext(ctx, "Thinktank binary not found", "binary_path", thinktankBinaryPath, "error", err)
		}
	} else {
		// For local development, find or build the binary
		logger.InfoContext(ctx, "Running in local development environment")
		var err error
		thinktankBinaryPath, err = findOrBuildBinary()
		if err != nil {
			logger.FatalContext(ctx, "Failed to find or build thinktank binary for E2E tests", "error", err)
		}
	}

	// Run all tests in the package
	logger.InfoContext(ctx, "Starting E2E test execution", "binary_path", thinktankBinaryPath)
	exitCode := m.Run()

	// Log test completion with results
	if exitCode == 0 {
		logger.InfoContext(ctx, "E2E tests completed successfully", "exit_code", exitCode)
	} else {
		logger.ErrorContext(ctx, "E2E tests completed with failures", "exit_code", exitCode)
	}

	// Perform any global cleanup here if needed
	// Note: we don't remove the binary as it might be reused in subsequent test runs

	// Exit with the status code from the test run
	os.Exit(exitCode)
}
